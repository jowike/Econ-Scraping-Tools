{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe0514c-914d-49c8-8af7-62f0e6788a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e95555a-5a9a-4e6a-bef0-acd4c032d4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downloading Selenium libraries \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# downloading BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# downloading Numpy & Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Downloading sleep function \n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdccb1f-ca83-4c64-8af5-05ab54d7bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting Selenium options \n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "# setting website that should be scraped \n",
    "driver.get(\"https://fred.stlouisfed.org/releases/calendar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4c4dfc-ea37-40d0-8344-b30c5a13c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    \n",
    "    \"\"\"\n",
    "    get_table function retrieves tables from the website, transforms it into a dataframe and \n",
    "    modifies it in order to obtain the most important information in a transparent way\n",
    "    \"\"\"\n",
    "        \n",
    "    html=driver.page_source\n",
    "    html = html.replace(\"N/A\",\"Unspecified\")\n",
    "    \n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # downloading the table from the website\n",
    "    div=soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'})\n",
    "    table=pd.read_html(str(div))\n",
    "    \n",
    "    # transforming the table to string\n",
    "    ans = np.array(table)\n",
    "    ans = (ans[0])\n",
    "    \n",
    "    # saving array as a dataframe \n",
    "    df = pd.DataFrame(ans, columns = ['Hour','Variable'])\n",
    "    \n",
    "    # saving new variable\n",
    "    globals()['last_day_of_the_week'] = df['Variable'].iloc[0]\n",
    "\n",
    "    # editing the dataframe\n",
    "    df['Hour'].fillna(method='pad', inplace=True)\n",
    "    df['Date'] = df['Variable'].iloc[0]\n",
    "    df = df.iloc[1: , :]\n",
    "    dfx = df['Date'].str.split(' ', expand=True)\n",
    "    df = df.drop('Date', axis=1)\n",
    "    df = pd.concat([dfx, df.reindex(dfx.index)], axis=1)\n",
    "    df[2] = df[2].replace(',','', regex=True)\n",
    "    df.rename(columns = {1:'Month', 2:'Day', 3:'Year'}, inplace = True)\n",
    "    get_table.df = df\n",
    "    \n",
    "    # saving the weekday of the table to be able to delte it in next step\n",
    "    weekday = df[0].iloc[0]\n",
    "    df = df.drop(0, axis=1)\n",
    "    \n",
    "    # saving results as a global df \n",
    "    globals()[f'df_{weekday}'] = df\n",
    "    \n",
    "    # writed to confirm the execution of the function \n",
    "    print(weekday + \", \" + df['Day'].iloc[0] + \" \" + df['Month'].iloc[0] + \" downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91cd4f-2e3d-49fa-9b80-77cc682f4472",
   "metadata": {},
   "source": [
    "<div id=\"release-dates-pager\">    No release dates are available for the selected options.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b356d9b-ba2d-4bb5-a033-5d8be712be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "\n",
    "def get_table():\n",
    "    \n",
    "    \"\"\"\n",
    "    get_table function retrieves tables from the website, transforms it into a dataframe and \n",
    "    modifies it in order to obtain the most important information in a transparent way\n",
    "    \"\"\"\n",
    "        \n",
    "    html=driver.page_source\n",
    "    html = html.replace(\"N/A\",\"Unspecified\")\n",
    "    \n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # downloading the table from the website\n",
    "    if soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'}):\n",
    "        div=soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'})\n",
    "        table=pd.read_html(str(div)) # tu się wywala błąd - tj. nie ma tabeli \n",
    "        \n",
    "        # transforming the table to string\n",
    "        ans = np.array(table)\n",
    "        ans = (ans[0])\n",
    "    \n",
    "        # saving array as a dataframe \n",
    "        df = pd.DataFrame(ans, columns = ['Hour','Variable'])\n",
    "    \n",
    "        # saving new variable\n",
    "        globals()['last_day_of_the_week'] = df['Variable'].iloc[0]\n",
    "\n",
    "        # editing the dataframe\n",
    "        df['Hour'].fillna(method='pad', inplace=True)\n",
    "        df['Date'] = df['Variable'].iloc[0]\n",
    "        df = df.iloc[1: , :]\n",
    "        dfx = df['Date'].str.split(' ', expand=True)\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df = pd.concat([dfx, df.reindex(dfx.index)], axis=1)\n",
    "        df[2] = df[2].replace(',','', regex=True)\n",
    "        df.rename(columns = {1:'Month', 2:'Day', 3:'Year'}, inplace = True)\n",
    "        get_table.df = df\n",
    "    \n",
    "        # saving the weekday of the table to be able to delte it in next step\n",
    "        weekday = df[0].iloc[0]\n",
    "        df = df.drop(0, axis=1)\n",
    "    \n",
    "        # saving results as a global df \n",
    "        globals()[f'df_{weekday}'] = df\n",
    "    \n",
    "        # writed to confirm the execution of the function \n",
    "        print(\"    \" + weekday + \", \" + df['Day'].iloc[0] + \" \" + df['Month'].iloc[0] + \" downloaded\")\n",
    "        \n",
    "    else:\n",
    "        print('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323cf31f-bd27-4adf-b80d-83e57c0a41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(name):\n",
    "    \n",
    "    \"\"\"\n",
    "    a function that combines a dataframe from individual days into one large dataframe for a whole week\n",
    "    \"\"\"\n",
    "    \n",
    "    # merging all df's to week_df\n",
    "    week_df = pd.concat([df_Sunday, df_Monday, df_Tuesday, df_Wednesday, df_Thursday, df_Friday, df_Saturday])\n",
    "    \n",
    "\n",
    "    week_df.rename(columns = {4:'Updated'}, inplace = True)\n",
    "    week_df = week_df.reset_index()\n",
    "    globals()[name] = week_df\n",
    "        \n",
    "    #(f'df_{last_day_of_the_week}.csv')\n",
    "    \n",
    "    #saving it as csv file\n",
    "    #week_df.to_csv(r\"C:\\Users\\ddawiec001\\Desktop\\FRED\\dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2099323-2ff6-4643-bb4f-fdecb43cb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait():\n",
    "    \n",
    "    \"\"\"\n",
    "    feature designed for delays allowing the full page loading\n",
    "    \"\"\"    \n",
    "    \n",
    "    WebDriverWait(driver,100).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"release-dates-pager\"]/div/table/tbody/tr[2]/td[2]')));\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01f7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wait(delay=100):\n",
    "    WebDriverWait(driver,delay).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='fc-next-button fc-button fc-state-default fc-corner-right']\")));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cfc0de5-7386-4c17-a3d9-b4aef57bc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week():\n",
    "    \n",
    "    \"\"\"\n",
    "    a function that combines all the previous functions together, \n",
    "    it retrieves data from the whole week and combines them into one dataframe\n",
    "    \"\"\"    \n",
    "    # setting Selenium buttons\n",
    "    WebDriverWait(driver,100).until(EC.element_to_be_clickable((By.XPATH, \"//*[@class='fc-next-button fc-button fc-state-default fc-corner-right']\")));\n",
    "    next_week = driver.find_element(By.XPATH, \"//*[@class='fc-next-button fc-button fc-state-default fc-corner-right']\")\n",
    "    fc_days = driver.find_elements(By.XPATH, \"//*[@class='fc-day-grid-event fc-h-event fc-event fc-start fc-end']\")\n",
    "    \n",
    "    fc_sun, fc_mon, fc_tue, fc_wed, fc_thu, fc_fri, fc_sat = fc_days\n",
    "    \n",
    "    # Executing the process\n",
    "    fc_sat.click();\n",
    "    wait();\n",
    "    \n",
    "    fc_sun.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    fc_mon.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    fc_tue.click();\n",
    "    wait();\n",
    "    get_table();  \n",
    "    \n",
    "    fc_wed.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    fc_thu.click();\n",
    "    wait();\n",
    "    get_table();  \n",
    "        \n",
    "    fc_fri.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    fc_sat.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    next_week.click();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4cbce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sunday, 21 July downloaded\n",
      "    Monday, 22 July downloaded\n",
      "    Tuesday, 23 July downloaded\n",
      "    Wednesday, 24 July downloaded\n",
      "    Thursday, 25 July downloaded\n",
      "    Friday, 26 July downloaded\n",
      "    Saturday, 27 July downloaded\n"
     ]
    }
   ],
   "source": [
    "week()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa764789-b3e2-4148-85c6-637fca4c717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data('week_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b09e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9de647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads for week_1\n",
      "  Attempt 1 for week_1\n",
      "    Sunday, 21 July downloaded\n",
      "    Monday, 22 July downloaded\n",
      "    Tuesday, 23 July downloaded\n",
      "    Wednesday, 24 July downloaded\n",
      "    Thursday, 25 July downloaded\n",
      "    Friday, 26 July downloaded\n",
      "    Saturday, 27 July downloaded\n",
      "  Successfully completed download for week_1 on attempt 1\n",
      "  Downloading data for week_1\n",
      "  Completed downloads for week_1\n",
      "Starting downloads for week_2\n",
      "  Attempt 1 for week_2\n",
      "  Attempt 1 failed for week_2, retrying...\n",
      "  Attempt 2 for week_2\n",
      "    Sunday, 28 July downloaded\n",
      "    Monday, 29 July downloaded\n",
      "    Tuesday, 30 July downloaded\n",
      "    Wednesday, 31 July downloaded\n",
      "    Thursday, 01 August downloaded\n",
      "    Friday, 02 August downloaded\n",
      "    Saturday, 03 August downloaded\n",
      "  Successfully completed download for week_2 on attempt 2\n",
      "  Downloading data for week_2\n",
      "  Completed downloads for week_2\n",
      "Starting downloads for week_3\n",
      "  Attempt 1 for week_3\n",
      "  Attempt 1 failed for week_3, retrying...\n",
      "  Attempt 2 for week_3\n",
      "    Sunday, 04 August downloaded\n",
      "    Monday, 05 August downloaded\n",
      "    Tuesday, 06 August downloaded\n",
      "    Wednesday, 07 August downloaded\n",
      "    Thursday, 08 August downloaded\n",
      "    Friday, 09 August downloaded\n",
      "    Saturday, 10 August downloaded\n",
      "  Successfully completed download for week_3 on attempt 2\n",
      "  Downloading data for week_3\n",
      "  Completed downloads for week_3\n",
      "Starting downloads for week_4\n",
      "  Attempt 1 for week_4\n",
      "  Attempt 1 failed for week_4, retrying...\n",
      "  Attempt 2 for week_4\n",
      "    Sunday, 11 August downloaded\n",
      "    Monday, 12 August downloaded\n",
      "    Tuesday, 13 August downloaded\n",
      "    Wednesday, 14 August downloaded\n",
      "    Thursday, 15 August downloaded\n",
      "    Friday, 16 August downloaded\n",
      "    Saturday, 17 August downloaded\n",
      "  Successfully completed download for week_4 on attempt 2\n",
      "  Downloading data for week_4\n",
      "  Completed downloads for week_4\n",
      "Starting downloads for week_5\n",
      "  Attempt 1 for week_5\n",
      "  Attempt 1 failed for week_5, retrying...\n",
      "  Attempt 2 for week_5\n",
      "    Sunday, 18 August downloaded\n",
      "    Monday, 19 August downloaded\n",
      "    Tuesday, 20 August downloaded\n",
      "    Wednesday, 21 August downloaded\n",
      "    Thursday, 22 August downloaded\n",
      "    Friday, 23 August downloaded\n",
      "    Saturday, 24 August downloaded\n",
      "  Successfully completed download for week_5 on attempt 2\n",
      "  Downloading data for week_5\n",
      "  Completed downloads for week_5\n",
      "Starting downloads for week_6\n",
      "  Attempt 1 for week_6\n",
      "  Attempt 1 failed for week_6, retrying...\n",
      "  Attempt 2 for week_6\n",
      "    Sunday, 25 August downloaded\n",
      "    Monday, 26 August downloaded\n",
      "    Tuesday, 27 August downloaded\n",
      "    Wednesday, 28 August downloaded\n",
      "    Thursday, 29 August downloaded\n",
      "    Friday, 30 August downloaded\n",
      "    Saturday, 31 August downloaded\n",
      "  Successfully completed download for week_6 on attempt 2\n",
      "  Downloading data for week_6\n",
      "  Completed downloads for week_6\n",
      "Starting downloads for week_7\n",
      "  Attempt 1 for week_7\n",
      "  Attempt 1 failed for week_7, retrying...\n",
      "  Attempt 2 for week_7\n",
      "    Sunday, 01 September downloaded\n",
      "    Monday, 02 September downloaded\n",
      "    Tuesday, 03 September downloaded\n",
      "    Wednesday, 04 September downloaded\n",
      "    Thursday, 05 September downloaded\n",
      "    Friday, 06 September downloaded\n",
      "    Saturday, 07 September downloaded\n",
      "  Successfully completed download for week_7 on attempt 2\n",
      "  Downloading data for week_7\n",
      "  Completed downloads for week_7\n",
      "Starting downloads for week_8\n",
      "  Attempt 1 for week_8\n",
      "  Attempt 1 failed for week_8, retrying...\n",
      "  Attempt 2 for week_8\n",
      "    Sunday, 08 September downloaded\n",
      "    Monday, 09 September downloaded\n",
      "    Tuesday, 10 September downloaded\n",
      "    Wednesday, 11 September downloaded\n",
      "    Thursday, 12 September downloaded\n",
      "    Friday, 13 September downloaded\n",
      "    Saturday, 14 September downloaded\n",
      "  Successfully completed download for week_8 on attempt 2\n",
      "  Downloading data for week_8\n",
      "  Completed downloads for week_8\n",
      "Starting downloads for week_9\n",
      "  Attempt 1 for week_9\n",
      "  Attempt 1 failed for week_9, retrying...\n",
      "  Attempt 2 for week_9\n",
      "    Sunday, 15 September downloaded\n",
      "    Monday, 16 September downloaded\n",
      "    Tuesday, 17 September downloaded\n",
      "    Wednesday, 18 September downloaded\n",
      "    Thursday, 19 September downloaded\n",
      "    Friday, 20 September downloaded\n",
      "    Saturday, 21 September downloaded\n",
      "  Successfully completed download for week_9 on attempt 2\n",
      "  Downloading data for week_9\n",
      "  Completed downloads for week_9\n",
      "Starting downloads for week_10\n",
      "  Attempt 1 for week_10\n",
      "  Attempt 1 failed for week_10, retrying...\n",
      "  Attempt 2 for week_10\n",
      "    Sunday, 22 September downloaded\n",
      "    Monday, 23 September downloaded\n",
      "    Tuesday, 24 September downloaded\n",
      "    Wednesday, 25 September downloaded\n",
      "    Thursday, 26 September downloaded\n",
      "    Friday, 27 September downloaded\n",
      "    Saturday, 28 September downloaded\n",
      "  Successfully completed download for week_10 on attempt 2\n",
      "  Downloading data for week_10\n",
      "  Completed downloads for week_10\n",
      "Starting downloads for week_11\n",
      "  Attempt 1 for week_11\n",
      "  Attempt 1 failed for week_11, retrying...\n",
      "  Attempt 2 for week_11\n",
      "    Sunday, 29 September downloaded\n",
      "    Monday, 30 September downloaded\n",
      "    Tuesday, 01 October downloaded\n",
      "    Wednesday, 02 October downloaded\n",
      "    Thursday, 03 October downloaded\n",
      "    Friday, 04 October downloaded\n",
      "    Saturday, 05 October downloaded\n",
      "  Successfully completed download for week_11 on attempt 2\n",
      "  Downloading data for week_11\n",
      "  Completed downloads for week_11\n",
      "Starting downloads for week_12\n",
      "  Attempt 1 for week_12\n",
      "  Attempt 1 failed for week_12, retrying...\n",
      "  Attempt 2 for week_12\n",
      "    Sunday, 06 October downloaded\n",
      "    Monday, 07 October downloaded\n",
      "    Tuesday, 08 October downloaded\n",
      "    Wednesday, 09 October downloaded\n",
      "    Thursday, 10 October downloaded\n",
      "    Friday, 11 October downloaded\n",
      "    Saturday, 12 October downloaded\n",
      "  Successfully completed download for week_12 on attempt 2\n",
      "  Downloading data for week_12\n",
      "  Completed downloads for week_12\n",
      "Starting downloads for week_13\n",
      "  Attempt 1 for week_13\n",
      "  Attempt 1 failed for week_13, retrying...\n",
      "  Attempt 2 for week_13\n",
      "    Sunday, 13 October downloaded\n",
      "    Monday, 14 October downloaded\n",
      "    Tuesday, 15 October downloaded\n",
      "    Wednesday, 16 October downloaded\n",
      "    Thursday, 17 October downloaded\n",
      "    Friday, 18 October downloaded\n",
      "    Saturday, 19 October downloaded\n",
      "  Successfully completed download for week_13 on attempt 2\n",
      "  Downloading data for week_13\n",
      "  Completed downloads for week_13\n",
      "Starting downloads for week_14\n",
      "  Attempt 1 for week_14\n",
      "  Attempt 1 failed for week_14, retrying...\n",
      "  Attempt 2 for week_14\n",
      "    Sunday, 20 October downloaded\n",
      "    Monday, 21 October downloaded\n",
      "    Tuesday, 22 October downloaded\n",
      "    Wednesday, 23 October downloaded\n",
      "    Thursday, 24 October downloaded\n",
      "    Friday, 25 October downloaded\n",
      "    Saturday, 26 October downloaded\n",
      "  Successfully completed download for week_14 on attempt 2\n",
      "  Downloading data for week_14\n",
      "  Completed downloads for week_14\n",
      "Starting downloads for week_15\n",
      "  Attempt 1 for week_15\n",
      "  Attempt 1 failed for week_15, retrying...\n",
      "  Attempt 2 for week_15\n",
      "    Sunday, 27 October downloaded\n",
      "    Monday, 28 October downloaded\n",
      "    Tuesday, 29 October downloaded\n",
      "    Wednesday, 30 October downloaded\n",
      "    Thursday, 31 October downloaded\n",
      "    Friday, 01 November downloaded\n",
      "    Saturday, 02 November downloaded\n",
      "  Successfully completed download for week_15 on attempt 2\n",
      "  Downloading data for week_15\n",
      "  Completed downloads for week_15\n",
      "Starting downloads for week_16\n",
      "  Attempt 1 for week_16\n",
      "  Attempt 1 failed for week_16, retrying...\n",
      "  Attempt 2 for week_16\n",
      "    Sunday, 03 November downloaded\n",
      "    Monday, 04 November downloaded\n",
      "    Tuesday, 05 November downloaded\n",
      "    Wednesday, 06 November downloaded\n",
      "    Thursday, 07 November downloaded\n",
      "    Friday, 08 November downloaded\n",
      "    Saturday, 09 November downloaded\n",
      "  Successfully completed download for week_16 on attempt 2\n",
      "  Downloading data for week_16\n",
      "  Completed downloads for week_16\n",
      "Starting downloads for week_17\n",
      "  Attempt 1 for week_17\n",
      "  Attempt 1 failed for week_17, retrying...\n",
      "  Attempt 2 for week_17\n",
      "    Sunday, 10 November downloaded\n",
      "    Monday, 11 November downloaded\n",
      "    Tuesday, 12 November downloaded\n",
      "    Wednesday, 13 November downloaded\n",
      "    Thursday, 14 November downloaded\n",
      "    Friday, 15 November downloaded\n",
      "    Saturday, 16 November downloaded\n",
      "  Successfully completed download for week_17 on attempt 2\n",
      "  Downloading data for week_17\n",
      "  Completed downloads for week_17\n",
      "Starting downloads for week_18\n",
      "  Attempt 1 for week_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed for week_18, retrying...\n",
      "  Attempt 2 for week_18\n",
      "    Sunday, 17 November downloaded\n",
      "    Monday, 18 November downloaded\n",
      "    Tuesday, 19 November downloaded\n",
      "    Wednesday, 20 November downloaded\n",
      "    Thursday, 21 November downloaded\n",
      "    Friday, 22 November downloaded\n",
      "    Saturday, 23 November downloaded\n",
      "  Successfully completed download for week_18 on attempt 2\n",
      "  Downloading data for week_18\n",
      "  Completed downloads for week_18\n",
      "Starting downloads for week_19\n",
      "  Attempt 1 for week_19\n",
      "  Attempt 1 failed for week_19, retrying...\n",
      "  Attempt 2 for week_19\n",
      "    Sunday, 24 November downloaded\n",
      "    Monday, 25 November downloaded\n",
      "    Tuesday, 26 November downloaded\n",
      "    Wednesday, 27 November downloaded\n",
      "    Thursday, 28 November downloaded\n",
      "    Friday, 29 November downloaded\n",
      "    Saturday, 30 November downloaded\n",
      "  Successfully completed download for week_19 on attempt 2\n",
      "  Downloading data for week_19\n",
      "  Completed downloads for week_19\n",
      "Starting downloads for week_20\n",
      "  Attempt 1 for week_20\n",
      "  Attempt 1 failed for week_20, retrying...\n",
      "  Attempt 2 for week_20\n",
      "    Sunday, 01 December downloaded\n",
      "    Monday, 02 December downloaded\n",
      "    Tuesday, 03 December downloaded\n",
      "    Wednesday, 04 December downloaded\n",
      "    Thursday, 05 December downloaded\n",
      "    Friday, 06 December downloaded\n",
      "    Saturday, 07 December downloaded\n",
      "  Successfully completed download for week_20 on attempt 2\n",
      "  Downloading data for week_20\n",
      "  Completed downloads for week_20\n",
      "Starting downloads for week_21\n",
      "  Attempt 1 for week_21\n",
      "  Attempt 1 failed for week_21, retrying...\n",
      "  Attempt 2 for week_21\n",
      "    Sunday, 08 December downloaded\n",
      "    Monday, 09 December downloaded\n",
      "    Tuesday, 10 December downloaded\n",
      "    Wednesday, 11 December downloaded\n",
      "    Thursday, 12 December downloaded\n",
      "    Friday, 13 December downloaded\n",
      "    Saturday, 14 December downloaded\n",
      "  Successfully completed download for week_21 on attempt 2\n",
      "  Downloading data for week_21\n",
      "  Completed downloads for week_21\n",
      "Starting downloads for week_22\n",
      "  Attempt 1 for week_22\n",
      "  Attempt 1 failed for week_22, retrying...\n",
      "  Attempt 2 for week_22\n",
      "    Sunday, 15 December downloaded\n",
      "    Monday, 16 December downloaded\n",
      "    Tuesday, 17 December downloaded\n",
      "    Wednesday, 18 December downloaded\n",
      "    Thursday, 19 December downloaded\n",
      "    Friday, 20 December downloaded\n",
      "    Saturday, 21 December downloaded\n",
      "  Successfully completed download for week_22 on attempt 2\n",
      "  Downloading data for week_22\n",
      "  Completed downloads for week_22\n",
      "Starting downloads for week_23\n",
      "  Attempt 1 for week_23\n",
      "  Attempt 1 failed for week_23, retrying...\n",
      "  Attempt 2 for week_23\n",
      "    Sunday, 22 December downloaded\n",
      "    Monday, 23 December downloaded\n",
      "    Tuesday, 24 December downloaded\n",
      "    Wednesday, 25 December downloaded\n",
      "    Thursday, 26 December downloaded\n",
      "    Friday, 27 December downloaded\n",
      "    Saturday, 28 December downloaded\n",
      "  Successfully completed download for week_23 on attempt 2\n",
      "  Downloading data for week_23\n",
      "  Completed downloads for week_23\n"
     ]
    }
   ],
   "source": [
    "max_retries = 30\n",
    "\n",
    "for week_i in [f'week_{i}' for i in range(1, 24)]:\n",
    "    print(f\"Starting downloads for {week_i}\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        print(f\"  Attempt {attempt + 1} for {week_i}\")\n",
    "        \n",
    "        try:\n",
    "            week()\n",
    "            print(f\"  Successfully completed download for {week_i} on attempt {attempt + 1}\")\n",
    "            break\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(f\"  Attempt {attempt + 1} failed for {week_i}, retrying...\")\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"rc-rid\"]/option[1]').click()\n",
    "            # sleep(60)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"  Downloading data for {week_i}\")\n",
    "    download_data(week_i)\n",
    "    print(f\"  Completed downloads for {week_i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fa53ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index     Month Day  Year  Updated         Hour  \\\n",
      "0        1      July  21  2024  Updated      7:00 pm   \n",
      "1        2      July  21  2024  Updated  Unspecified   \n",
      "2        3      July  21  2024  Updated  Unspecified   \n",
      "3        1      July  22  2024  Updated      1:00 am   \n",
      "4        2      July  22  2024  Updated      2:00 am   \n",
      "..     ...       ...  ..   ...      ...          ...   \n",
      "175     39  December  20  2024      NaN  Unspecified   \n",
      "176     40  December  20  2024      NaN  Unspecified   \n",
      "177     41  December  20  2024      NaN  Unspecified   \n",
      "178      1  December  21  2024      NaN      7:00 pm   \n",
      "179      2  December  21  2024      NaN  Unspecified   \n",
      "\n",
      "                                              Variable  \n",
      "0                            Coinbase Cryptocurrencies  \n",
      "1                               Equifax Credit Quality  \n",
      "2                                   FOMC Press Release  \n",
      "3                                 Euro Short Term Rate  \n",
      "4     Swiss National Bank Monthly Statistical Bulletin  \n",
      "..                                                 ...  \n",
      "175                                   Standard & Poors  \n",
      "176                                     Nikkei Indexes  \n",
      "177  Supplemental Estimates, Underlying Detail Tabl...  \n",
      "178                          Coinbase Cryptocurrencies  \n",
      "179                                 FOMC Press Release  \n",
      "\n",
      "[3720 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "all_weeks_df = pd.concat([week_1, week_2, week_3, week_4, week_5, week_6, week_7, week_8, week_9, week_10, week_11, week_12, week_13, week_14, week_15, week_16, week_17, week_18, week_19, week_20, week_21, week_22])\n",
    "    \n",
    "print(all_weeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fad05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving it as csv file\n",
    "all_weeks_df.to_csv(r\"./FRED/all_weeks_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6d503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
