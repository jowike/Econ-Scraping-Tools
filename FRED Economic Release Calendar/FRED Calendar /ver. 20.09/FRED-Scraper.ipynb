{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe0514c-914d-49c8-8af7-62f0e6788a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e95555a-5a9a-4e6a-bef0-acd4c032d4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downloading Selenium libraries \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# downloading BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# downloading Numpy & Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Downloading sleep function \n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdccb1f-ca83-4c64-8af5-05ab54d7bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting Selenium options \n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "# setting website that should be scraped \n",
    "driver.get(\"https://fred.stlouisfed.org/releases/calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4c4dfc-ea37-40d0-8344-b30c5a13c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    \n",
    "    \"\"\"\n",
    "    get_table function retrieves tables from the website, transforms it into a dataframe and \n",
    "    modifies it in order to obtain the most important information in a transparent way\n",
    "    \"\"\"\n",
    "        \n",
    "    html=driver.page_source\n",
    "    html = html.replace(\"N/A\",\"Unspecified\")\n",
    "    \n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # downloading the table from the website\n",
    "    div=soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'})\n",
    "    table=pd.read_html(str(div))\n",
    "    \n",
    "    # transforming the table to string\n",
    "    ans = np.array(table)\n",
    "    ans = (ans[0])\n",
    "    \n",
    "    # saving array as a dataframe \n",
    "    df = pd.DataFrame(ans, columns = ['Hour','Variable'])\n",
    "    \n",
    "    # saving new variable\n",
    "    globals()['last_day_of_the_week'] = df['Variable'].iloc[0]\n",
    "\n",
    "    # editing the dataframe\n",
    "    df['Hour'].fillna(method='pad', inplace=True)\n",
    "    df['Date'] = df['Variable'].iloc[0]\n",
    "    df = df.iloc[1: , :]\n",
    "    dfx = df['Date'].str.split(' ', expand=True)\n",
    "    df = df.drop('Date', axis=1)\n",
    "    df = pd.concat([dfx, df.reindex(dfx.index)], axis=1)\n",
    "    df[2] = df[2].replace(',','', regex=True)\n",
    "    df.rename(columns = {1:'Month', 2:'Day', 3:'Year'}, inplace = True)\n",
    "    get_table.df = df\n",
    "    \n",
    "    # saving the weekday of the table to be able to delte it in next step\n",
    "    weekday = df[0].iloc[0]\n",
    "    df = df.drop(0, axis=1)\n",
    "    \n",
    "    # saving results as a global df \n",
    "    globals()[f'df_{weekday}'] = df\n",
    "    \n",
    "    # writed to confirm the execution of the function \n",
    "    print(weekday + \", \" + df['Day'].iloc[0] + \" \" + df['Month'].iloc[0] + \" downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91cd4f-2e3d-49fa-9b80-77cc682f4472",
   "metadata": {},
   "source": [
    "<div id=\"release-dates-pager\">    No release dates are available for the selected options.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b356d9b-ba2d-4bb5-a033-5d8be712be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "\n",
    "def get_table():\n",
    "    \n",
    "    \"\"\"\n",
    "    get_table function retrieves tables from the website, transforms it into a dataframe and \n",
    "    modifies it in order to obtain the most important information in a transparent way\n",
    "    \"\"\"\n",
    "        \n",
    "    html=driver.page_source\n",
    "    html = html.replace(\"N/A\",\"Unspecified\")\n",
    "    \n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # downloading the table from the website\n",
    "    if soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'}):\n",
    "        div=soup.find_all('table', attrs={'class':'table table-condensed table-standard-theme'})\n",
    "        table=pd.read_html(str(div)) # tu się wywala błąd - tj. nie ma tabeli \n",
    "        \n",
    "        # transforming the table to string\n",
    "        ans = np.array(table)\n",
    "        ans = (ans[0])\n",
    "    \n",
    "        # saving array as a dataframe \n",
    "        df = pd.DataFrame(ans, columns = ['Hour','Variable'])\n",
    "    \n",
    "        # saving new variable\n",
    "        globals()['last_day_of_the_week'] = df['Variable'].iloc[0]\n",
    "\n",
    "        # editing the dataframe\n",
    "        df['Hour'].fillna(method='pad', inplace=True)\n",
    "        df['Date'] = df['Variable'].iloc[0]\n",
    "        df = df.iloc[1: , :]\n",
    "        dfx = df['Date'].str.split(' ', expand=True)\n",
    "        df = df.drop('Date', axis=1)\n",
    "        df = pd.concat([dfx, df.reindex(dfx.index)], axis=1)\n",
    "        df[2] = df[2].replace(',','', regex=True)\n",
    "        df.rename(columns = {1:'Month', 2:'Day', 3:'Year'}, inplace = True)\n",
    "        get_table.df = df\n",
    "    \n",
    "        # saving the weekday of the table to be able to delte it in next step\n",
    "        weekday = df[0].iloc[0]\n",
    "        df = df.drop(0, axis=1)\n",
    "    \n",
    "        # saving results as a global df \n",
    "        globals()[f'df_{weekday}'] = df\n",
    "    \n",
    "        # writed to confirm the execution of the function \n",
    "        print(weekday + \", \" + df['Day'].iloc[0] + \" \" + df['Month'].iloc[0] + \" downloaded\")\n",
    "        \n",
    "    else:\n",
    "        print('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323cf31f-bd27-4adf-b80d-83e57c0a41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(name):\n",
    "    \n",
    "    \"\"\"\n",
    "    a function that combines a dataframe from individual days into one large dataframe for a whole week\n",
    "    \"\"\"\n",
    "    \n",
    "    # merging all df's to week_df\n",
    "    week_df = pd.concat([df_Sunday, df_Monday, df_Tuesday, df_Wednesday, df_Thursday, df_Friday, df_Saturday])\n",
    "    \n",
    "\n",
    "    week_df.rename(columns = {4:'Updated'}, inplace = True)\n",
    "    week_df = week_df.reset_index()\n",
    "    globals()[name] = week_df\n",
    "        \n",
    "    #(f'df_{last_day_of_the_week}.csv')\n",
    "    \n",
    "    #saving it as csv file\n",
    "    #week_df.to_csv(r\"C:\\Users\\ddawiec001\\Desktop\\FRED\\dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2099323-2ff6-4643-bb4f-fdecb43cb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait():\n",
    "    \n",
    "    \"\"\"\n",
    "    feature designed for delays allowing the full page loading\n",
    "    \"\"\"\n",
    "    \n",
    "    WebDriverWait(driver,100).until(EC.presence_of_element_located((By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td\")));\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfc0de5-7386-4c17-a3d9-b4aef57bc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week():\n",
    "    \n",
    "    \"\"\"\n",
    "    a function that combines all the previous functions together, \n",
    "    it retrieves data from the whole week and combines them into one dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # setting Selenium buttons\n",
    "    next_week = driver.find_element(By.XPATH, \"//span[text()='›']\")\n",
    "    sunday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[1]\")\n",
    "    monday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[2]\")\n",
    "    tuesday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[3]\")\n",
    "    wednesday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[4]\")\n",
    "    thursday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[5]\")\n",
    "    friday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[6]\")\n",
    "    saturday = driver.find_element(By.XPATH, \"//table[@class='fc-border-separate']/tbody/tr/td[7]\")\n",
    "    \n",
    "    # Executing the process\n",
    "    saturday.click();\n",
    "    wait();\n",
    "    \n",
    "    sunday.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    monday.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    tuesday.click();\n",
    "    wait();\n",
    "    get_table();  \n",
    "    \n",
    "    wednesday.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    thursday.click();\n",
    "    wait();\n",
    "    get_table();  \n",
    "        \n",
    "    friday.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    saturday.click();\n",
    "    wait();\n",
    "    get_table();\n",
    "    \n",
    "    next_week.click();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f14aae6-5f50-4cf7-a9fe-fdf054ae7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday, 04 December downloaded\n",
      "Monday, 05 December downloaded\n",
      "Tuesday, 06 December downloaded\n",
      "Wednesday, 07 December downloaded\n",
      "Thursday, 08 December downloaded\n",
      "Friday, 09 December downloaded\n",
      "Saturday, 10 December downloaded\n"
     ]
    }
   ],
   "source": [
    "week()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa764789-b3e2-4148-85c6-637fca4c717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data('week_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671e894e-7c9c-495f-afe3-3c5e930a08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday, 11 December downloaded\n",
      "Monday, 12 December downloaded\n",
      "Tuesday, 13 December downloaded\n",
      "Wednesday, 14 December downloaded\n",
      "Thursday, 15 December downloaded\n",
      "Friday, 16 December downloaded\n",
      "Saturday, 17 December downloaded\n",
      "Sunday, 18 December downloaded\n",
      "Monday, 19 December downloaded\n",
      "Tuesday, 20 December downloaded\n",
      "Wednesday, 21 December downloaded\n",
      "Thursday, 22 December downloaded\n",
      "Friday, 23 December downloaded\n",
      "Saturday, 24 December downloaded\n",
      "Sunday, 25 December downloaded\n",
      "Monday, 26 December downloaded\n",
      "Tuesday, 27 December downloaded\n",
      "Wednesday, 28 December downloaded\n",
      "Thursday, 29 December downloaded\n",
      "Friday, 30 December downloaded\n",
      "Saturday, 31 December downloaded\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "Thursday, 05 January downloaded\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "Tuesday, 10 January downloaded\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=107.0.5304.121)\nStacktrace:\n0   chromedriver                        0x0000000100f502c8 chromedriver + 4752072\n1   chromedriver                        0x0000000100ed0463 chromedriver + 4228195\n2   chromedriver                        0x0000000100b33b18 chromedriver + 441112\n3   chromedriver                        0x0000000100b10210 chromedriver + 295440\n4   chromedriver                        0x0000000100b95e3d chromedriver + 843325\n5   chromedriver                        0x0000000100ba9719 chromedriver + 923417\n6   chromedriver                        0x0000000100b91b33 chromedriver + 826163\n7   chromedriver                        0x0000000100b629fd chromedriver + 633341\n8   chromedriver                        0x0000000100b64051 chromedriver + 639057\n9   chromedriver                        0x0000000100f1d30e chromedriver + 4543246\n10  chromedriver                        0x0000000100f21a88 chromedriver + 4561544\n11  chromedriver                        0x0000000100f296df chromedriver + 4593375\n12  chromedriver                        0x0000000100f228fa chromedriver + 4565242\n13  chromedriver                        0x0000000100ef82cf chromedriver + 4391631\n14  chromedriver                        0x0000000100f415b8 chromedriver + 4691384\n15  chromedriver                        0x0000000100f41739 chromedriver + 4691769\n16  chromedriver                        0x0000000100f5781e chromedriver + 4782110\n17  libsystem_pthread.dylib             0x00007ff802a454e1 _pthread_start + 125\n18  libsystem_pthread.dylib             0x00007ff802a40f6b thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m week()\n\u001b[1;32m     14\u001b[0m download_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek_5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mweek\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m download_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek_6\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m week()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mweek\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m monday\u001b[38;5;241m.\u001b[39mclick();\n\u001b[1;32m     27\u001b[0m wait();\n\u001b[0;32m---> 28\u001b[0m \u001b[43mget_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     30\u001b[0m tuesday\u001b[38;5;241m.\u001b[39mclick();\n\u001b[1;32m     31\u001b[0m wait();\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mget_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_table\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    get_table function retrieves tables from the website, transforms it into a dataframe and \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    modifies it in order to obtain the most important information in a transparent way\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     html\u001b[38;5;241m=\u001b[39m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\n\u001b[1;32m     11\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnspecified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     soup\u001b[38;5;241m=\u001b[39mBeautifulSoup(html,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:550\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:444\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    442\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:249\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    247\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=107.0.5304.121)\nStacktrace:\n0   chromedriver                        0x0000000100f502c8 chromedriver + 4752072\n1   chromedriver                        0x0000000100ed0463 chromedriver + 4228195\n2   chromedriver                        0x0000000100b33b18 chromedriver + 441112\n3   chromedriver                        0x0000000100b10210 chromedriver + 295440\n4   chromedriver                        0x0000000100b95e3d chromedriver + 843325\n5   chromedriver                        0x0000000100ba9719 chromedriver + 923417\n6   chromedriver                        0x0000000100b91b33 chromedriver + 826163\n7   chromedriver                        0x0000000100b629fd chromedriver + 633341\n8   chromedriver                        0x0000000100b64051 chromedriver + 639057\n9   chromedriver                        0x0000000100f1d30e chromedriver + 4543246\n10  chromedriver                        0x0000000100f21a88 chromedriver + 4561544\n11  chromedriver                        0x0000000100f296df chromedriver + 4593375\n12  chromedriver                        0x0000000100f228fa chromedriver + 4565242\n13  chromedriver                        0x0000000100ef82cf chromedriver + 4391631\n14  chromedriver                        0x0000000100f415b8 chromedriver + 4691384\n15  chromedriver                        0x0000000100f41739 chromedriver + 4691769\n16  chromedriver                        0x0000000100f5781e chromedriver + 4782110\n17  libsystem_pthread.dylib             0x00007ff802a454e1 _pthread_start + 125\n18  libsystem_pthread.dylib             0x00007ff802a40f6b thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "week()\n",
    "download_data('week_1')\n",
    "\n",
    "week()\n",
    "download_data('week_2')\n",
    "\n",
    "week()\n",
    "download_data('week_3')\n",
    "\n",
    "week()\n",
    "download_data('week_4')\n",
    "\n",
    "week()\n",
    "download_data('week_5')\n",
    "\n",
    "week()\n",
    "download_data('week_6')\n",
    "\n",
    "week()\n",
    "download_data('week_7')\n",
    "\n",
    "week()\n",
    "download_data('week_8')\n",
    "\n",
    "week()\n",
    "download_data('week_9')\n",
    "\n",
    "week()\n",
    "download_data('week_10')\n",
    "\n",
    "week()\n",
    "download_data('week_11')\n",
    "\n",
    "week()\n",
    "download_data('week_12')\n",
    "\n",
    "week()\n",
    "download_data('week_13')\n",
    "\n",
    "week()\n",
    "download_data('week_14')\n",
    "\n",
    "week()\n",
    "download_data('week_15')\n",
    "\n",
    "week()\n",
    "download_data('week_16')\n",
    "\n",
    "week()\n",
    "download_data('week_17')\n",
    "\n",
    "week()\n",
    "download_data('week_18')\n",
    "\n",
    "week()\n",
    "download_data('week_19')\n",
    "\n",
    "week()\n",
    "download_data('week_20')\n",
    "\n",
    "week()\n",
    "download_data('week_21')\n",
    "\n",
    "week()\n",
    "download_data('week_22')\n",
    "\n",
    "week()\n",
    "download_data('week_23')\n",
    "\n",
    "week()\n",
    "download_data('week_24')\n",
    "\n",
    "\n",
    "all_weeks_df = pd.concat([week_1, week_2, week_3, week_4, week_5, week_6, week_7, week_8, week_9, week_10, week_11, week_12, week_13, week_14, week_15, week_16, week_17, week_18, week_19, week_20, week_21, week_22, week_23, week_24, ])\n",
    "    \n",
    "    \n",
    "#saving it as csv file\n",
    "all_weeks_df.to_csv(r\"./FRED/all_weeks_dataframe.csv\")\n",
    "\n",
    "print(all_weeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084db79-6218-4d3f-861f-cef1fe206e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
